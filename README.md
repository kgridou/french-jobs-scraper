# French Job Postings Data Engineering Pipeline

A production-ready, Docker-based data engineering portfolio project that demonstrates web scraping, data processing, and workflow orchestration for French job market analysis.

## üéØ Project Overview

This project scrapes job postings from French job boards (Indeed.fr, LinkedIn), processes the data using Pandas and PySpark, and orchestrates the entire pipeline with Apache Airflow. The system stores structured data in PostgreSQL and provides analytics-ready datasets.

## üèóÔ∏è Architecture

```
Web Scraping ‚Üí Raw Data Storage ‚Üí Data Processing ‚Üí Analytics Database
    (Python)      (PostgreSQL)      (Pandas/PySpark)    (PostgreSQL)
                         ‚Üë
                    Orchestration
                     (Airflow)
```

## üöÄ Features

- **Web Scraping**: Ethical scraping of French job postings with rate limiting
- **Data Processing**: ETL pipeline using Pandas for small datasets and PySpark for large-scale processing
- **Workflow Orchestration**: Automated daily scraping and processing with Airflow
- **Data Storage**: PostgreSQL database with normalized schema
- **Monitoring**: Airflow UI for pipeline monitoring and job status
- **Dockerized**: Fully containerized for easy deployment

## üìã Prerequisites

- Docker & Docker Compose
- 4GB+ RAM available
- Internet connection

## üîß Quick Start

1. **Clone and navigate to project**:
```bash
cd french-jobs-scraper
```

2. **Start the entire stack**:
```bash
docker-compose up -d
```

3. **Wait for services to initialize** (30-60 seconds):
```bash
docker-compose logs -f airflow-init
```

4. **Access services**:
- Airflow UI: http://localhost:8080 (username: `airflow`, password: `airflow`)
- Spark Master UI: http://localhost:8081
- PostgreSQL: localhost:5432 (user: `airflow`, password: `airflow`, db: `jobs_db`)

5. **Trigger the pipeline**:
- Navigate to Airflow UI
- Enable the `french_jobs_pipeline` DAG
- Trigger manually or wait for scheduled run (daily at 2 AM)

## üìÇ Project Structure

```
french-jobs-scraper/
‚îú‚îÄ‚îÄ dags/                             # Airflow DAG definitions
‚îÇ   ‚îî‚îÄ‚îÄ french_jobs_pipeline.py
‚îú‚îÄ‚îÄ scrapers/                         # Web scraper implementations
‚îÇ   ‚îú‚îÄ‚îÄ base_scraper.py
‚îÇ   ‚îú‚îÄ‚îÄ indeed_scraper.py
‚îÇ   ‚îî‚îÄ‚îÄ hellowork_scraper.py
‚îú‚îÄ‚îÄ scripts/                          # Data processing and utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py
‚îÇ   ‚îú‚îÄ‚îÄ pandas_processor.py
‚îÇ   ‚îú‚îÄ‚îÄ spark_processor.py
‚îÇ   ‚îú‚îÄ‚îÄ enrich_jobs.py
‚îÇ   ‚îî‚îÄ‚îÄ shell/                        # Shell scripts
‚îÇ       ‚îú‚îÄ‚îÄ start.sh
‚îÇ       ‚îú‚îÄ‚îÄ stop.sh
‚îÇ       ‚îî‚îÄ‚îÄ cleanup.sh
‚îú‚îÄ‚îÄ sql/                              # Database schema and queries
‚îÇ   ‚îú‚îÄ‚îÄ init.sql
‚îÇ   ‚îú‚îÄ‚îÄ sample_queries.sql
‚îÇ   ‚îî‚îÄ‚îÄ analytics_queries.sql
‚îú‚îÄ‚îÄ config/                           # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ scraper_config.yaml
‚îú‚îÄ‚îÄ docs/                             # Additional documentation
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ   ‚îú‚îÄ‚îÄ QUICKSTART.md
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ data/                             # Data storage (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ analytics/
‚îú‚îÄ‚îÄ docker-compose.yml                # Docker services orchestration
‚îú‚îÄ‚îÄ Dockerfile                        # Custom Airflow image
‚îú‚îÄ‚îÄ requirements.txt                  # Python dependencies
‚îî‚îÄ‚îÄ README.md                         # This file
```

## üîç Data Pipeline Details

### Stage 1: Web Scraping
- Scrapes Indeed.fr and LinkedIn France
- Extracts: job title, company, location, salary, description, posting date
- Implements rate limiting and user-agent rotation
- Handles French language and special characters

### Stage 2: Data Cleaning (Pandas)
- Removes duplicates
- Standardizes location names
- Parses salary ranges
- Extracts job categories
- Handles missing values

### Stage 3: Data Processing (PySpark)
- Aggregates job statistics by location and category
- Calculates salary trends
- Performs text analysis on job descriptions
- Creates analytics tables

### Stage 4: Storage
- Normalized database schema
- Fact and dimension tables
- Indexes for query performance

## üìä Database Schema

```sql
raw_jobs:           Scraped job postings (raw data)
cleaned_jobs:       Processed job data
job_categories:     Dimension table for categories
locations:          Dimension table for locations
companies:          Dimension table for companies
job_analytics:      Aggregated statistics
salary_trends:      Salary analysis by role/location
```

## üõ†Ô∏è Technologies Used

- **Python 3.11**: Core programming language
- **Apache Airflow 2.8**: Workflow orchestration
- **Apache Spark 3.5**: Large-scale data processing
- **Pandas**: Data manipulation and analysis
- **BeautifulSoup4 & Requests**: Web scraping
- **PostgreSQL 15**: Data storage
- **Docker & Docker Compose**: Containerization

## üìà Usage Examples

### Query Recent Jobs
```sql
SELECT title, company, location, salary_min, salary_max
FROM cleaned_jobs
WHERE posting_date >= CURRENT_DATE - INTERVAL '7 days'
ORDER BY posting_date DESC;
```

### Analyze Salary by City
```sql
SELECT location, AVG(salary_avg) as avg_salary, COUNT(*) as job_count
FROM job_analytics
GROUP BY location
ORDER BY avg_salary DESC;
```

### Top Hiring Companies
```sql
SELECT company, COUNT(*) as job_count
FROM cleaned_jobs
WHERE posting_date >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY company
ORDER BY job_count DESC
LIMIT 10;
```

## üîê Configuration

Edit `config/scraper_config.yaml` to customize:
- Target job sites
- Search keywords
- Geographic filters
- Scraping frequency
- Data retention policies

## üêõ Troubleshooting

**Airflow webserver not starting**:
```bash
docker-compose logs airflow-webserver
docker-compose restart airflow-webserver
```

**Database connection issues**:
```bash
docker-compose exec postgres psql -U airflow -d jobs_db
```

**Check Spark jobs**:
Visit http://localhost:8081 for Spark Master UI

## üìö Documentation

Additional documentation is available in the `docs/` directory:
- **[QUICKSTART.md](docs/QUICKSTART.md)** - Detailed step-by-step setup guide
- **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** - System architecture and design
- **[CI_CD.md](docs/CI_CD.md)** - CI/CD pipeline with GitHub Actions
- **[STRUCTURE.md](docs/STRUCTURE.md)** - Complete project structure details
- **[PROJECT_SUMMARY.md](docs/PROJECT_SUMMARY.md)** - Project overview and features
- **[GETTING_STARTED.md](docs/GETTING_STARTED.md)** - Getting started guide
- **[SETUP_NOTES.md](docs/SETUP_NOTES.md)** - Additional setup notes

For AI-assisted development, see **[CLAUDE.md](CLAUDE.md)** in the root directory.

## üîÑ CI/CD

This project includes comprehensive GitHub Actions workflows:

- **CI Pipeline** - Linting, testing, security scanning
- **Pipeline Integration Test** - Full end-to-end testing with actual services (dogfooding üêï)
- **Docker Build** - Automated container image builds and publishing

See [docs/CI_CD.md](docs/CI_CD.md) for details.

## üìù Development

To add new scrapers or processors:

1. Create new scraper in `scrapers/` inheriting from `base_scraper.py`
2. Add processing scripts in `scripts/`
3. Update Airflow DAG in `dags/french_jobs_pipeline.py`
4. Rebuild containers: `docker-compose up -d --build`

## ü§ù Portfolio Showcase

This project demonstrates:
- Modern data engineering practices
- ETL pipeline design
- Workflow orchestration
- Containerization
- Python best practices
- Data modeling
- Distributed computing with Spark

## ‚öñÔ∏è Legal & Ethics

This project is for educational and portfolio purposes. When scraping:
- Respect robots.txt
- Implement rate limiting
- Don't overload servers
- Follow terms of service
- Consider using official APIs when available

## üìÑ License

MIT License - Feel free to use for learning and portfolio purposes.
