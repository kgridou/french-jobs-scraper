name: Pipeline Integration Test

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run every Monday at 6 AM UTC
    - cron: '0 6 * * 1'
  workflow_dispatch:  # Allow manual trigger

jobs:
  test-pipeline:
    name: Test Full Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Create required directories
      run: |
        mkdir -p data/raw data/processed data/analytics logs
        mkdir -p dags scrapers scripts sql config

    - name: Create .env file
      run: |
        cp .env.example .env
        echo "AIRFLOW_UID=$(id -u)" >> .env

    - name: Start services with docker-compose
      run: |
        docker-compose up -d
        echo "Waiting for services to be ready..."
        sleep 30

    - name: Wait for PostgreSQL
      run: |
        timeout=60
        while ! docker-compose exec -T postgres pg_isready -U airflow > /dev/null 2>&1; do
          echo "Waiting for PostgreSQL..."
          sleep 2
          timeout=$((timeout-2))
          if [ $timeout -le 0 ]; then
            echo "PostgreSQL did not start in time"
            docker-compose logs postgres
            exit 1
          fi
        done
        echo "✓ PostgreSQL is ready"

    - name: Wait for Airflow webserver
      run: |
        timeout=120
        while ! curl -sf http://localhost:8080/health > /dev/null 2>&1; do
          echo "Waiting for Airflow webserver..."
          sleep 3
          timeout=$((timeout-3))
          if [ $timeout -le 0 ]; then
            echo "Airflow webserver did not start in time"
            docker-compose logs airflow-webserver
            exit 1
          fi
        done
        echo "✓ Airflow webserver is ready"

    - name: Check Spark Master
      run: |
        timeout=60
        while ! curl -sf http://localhost:8081 > /dev/null 2>&1; do
          echo "Waiting for Spark Master..."
          sleep 2
          timeout=$((timeout-2))
          if [ $timeout -le 0 ]; then
            echo "Warning: Spark Master may not be ready"
            break
          fi
        done
        echo "✓ Spark Master is accessible"

    - name: Verify database schema
      run: |
        docker-compose exec -T postgres psql -U airflow -d jobs_db -c "\dt" || {
          echo "Database tables not found, checking init.sql execution"
          docker-compose logs postgres
          exit 1
        }

    - name: List Airflow DAGs
      run: |
        docker-compose exec -T airflow-webserver airflow dags list

    - name: Test DAG validity
      run: |
        docker-compose exec -T airflow-webserver airflow dags list-import-errors

    - name: Run scraper test (minimal data)
      run: |
        # Test the scraper with limited pages
        docker-compose exec -T airflow-webserver python /opt/airflow/scripts/scraper.py || {
          echo "Scraper test failed, but continuing..."
        }
      continue-on-error: true

    - name: Trigger pipeline DAG (dry run)
      run: |
        # Just validate the DAG can be triggered, don't wait for completion
        docker-compose exec -T airflow-scheduler airflow dags test french_jobs_pipeline $(date +%Y-%m-%d) || {
          echo "DAG test failed, checking logs..."
          docker-compose logs airflow-scheduler
        }
      continue-on-error: true

    - name: Check for DAG errors
      run: |
        docker-compose exec -T airflow-webserver airflow dags list-import-errors | grep -i "french_jobs_pipeline" && {
          echo "DAG has import errors!"
          exit 1
        } || {
          echo "✓ No import errors in french_jobs_pipeline DAG"
        }

    - name: Verify data directories
      run: |
        ls -la data/raw/ data/processed/ data/analytics/ || echo "Data directories created"

    - name: Show service status
      run: |
        docker-compose ps

    - name: Show logs on failure
      if: failure()
      run: |
        echo "=== Airflow Webserver Logs ==="
        docker-compose logs --tail=100 airflow-webserver
        echo "=== Airflow Scheduler Logs ==="
        docker-compose logs --tail=100 airflow-scheduler
        echo "=== PostgreSQL Logs ==="
        docker-compose logs --tail=50 postgres
        echo "=== Spark Master Logs ==="
        docker-compose logs --tail=50 spark-master

    - name: Cleanup
      if: always()
      run: |
        docker-compose down -v
        docker system prune -f

  test-scripts:
    name: Test Individual Scripts
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Test scraper imports
      run: |
        python -c "from scrapers.base_scraper import BaseScraper; print('✓ BaseScraper imports successfully')"
        python -c "from scrapers.indeed_scraper import IndeedScraper; print('✓ IndeedScraper imports successfully')"
        python -c "from scrapers.hellowork_scraper import HelloWorkScraper; print('✓ HelloWorkScraper imports successfully')"
      continue-on-error: true

    - name: Test configuration loading
      run: |
        python -c "import yaml; config = yaml.safe_load(open('config/scraper_config.yaml')); print('✓ Configuration loaded successfully')"

    - name: Validate Python syntax
      run: |
        python -m py_compile scrapers/*.py
        python -m py_compile scripts/*.py
        python -m py_compile dags/*.py
        echo "✓ All Python files have valid syntax"
